```{r}
##-----Code Summary-----
options(repr.plot.width = 12, repr.plot.height = 6)  ## representation.

set.seed(number)  ## setting seed of curculation
lm(y~x)  ## This expression can be used only x and y are already defined.
bquote(beta[0]~'ㅋㅋ'~beta[1])  ## 인용하다. It can be arranging some expression using r-string, and wraping as a '~'
expression(beta[0])

```

```{r}
options(repr.plot.width = 12, repr.plot.height = 6)
```

> beta1과 beta0, 데이터의 수에 따라 수가 어떻게 바뀌는지 알아보자.

```{r}
num_obs <- 30
beta0 <- 10
beta1 <- -3
sigma <- 3
```

```{r}
set.seed(1)  ## 시드 설정
epsilon <- rnorm(n = num_obs, mean = 0, sd = sigma)
x <- 1:num_obs  ## striding
y <- beta0 + beta1*x + epsilon  ## 모회귀모형을 만들어봤음
```


```{r}
plot(x, y, pch = 16, col = 'darkorange', lwd = 2, cex = 0.5)
```

```{r}
lm_fit <- lm(y~x)  ## 반응변수를 항상 앞으로, 여기선 데이터를 지정해줄 필요가 없는데, 이미 y랑 x가 데이터임
summary(lm_fit)
```
```{r}
plot(x, y, pch = 16, col = 'darkorange', cex = 0.5)
abline(lm_fit, col = 'steelblue', lwd = 2)
```

> 잘 적합함. 그러면 이제 표준오차가 달라질 경우 어떻게 변하는지 알아보자.

\- sigma가 10일 때.

```{r}
num_obs <- 30
beta0 <- 10
beta1 <- -3
sigma2 <- 10

epsilon2 <- rnorm(n = num_obs, mean = 0, sd = sigma2)
y2 <- beta0 + beta1*x + epsilon2
```

```{r}
sim_model2 <- lm(y2~x)
summary(sim_model2)
```

```{r}
num_obs <- 30
beta0 <- 10
beta1 <- -3
sigma3 <- 2

epsilon3 <- rnorm(n = num_obs, mean = 0, sd = sigma3)
y3 <- beta0 + beta1*x + epsilon3
```

```{r}
sim_model3 <- lm(y3~x)
summary(sim_model3)
```

> 둘다 예측한 값은 비슷하다. True와의 차이가 별로 크지가 않음...

> sim_model3가 sim_model2보다 훨씬 유의확률이 낮아 더 잘 적합이 된다. 표준오차가 훨씬 작다... 즉, 자료의 분산이 작을수록 모델이 훨씬 더 유의해진다.

> 그냥 R^2도, F통계량도 훨씬 좋음.

```{r}
par(mfrow=c(1,2))  ## 그래프를 두 개 그리는 메소드인가봄(plt.subplots()???)
## graphical parameters, drawing multiple plot, multiple funcion ascending row or columns(mfcol)

plot(y2 ~ x, pch=16, col='darkorange',
 main =bquote(beta[0] ~ "="~.(beta1) ~ ", " ~
 beta[1] ~ "="~.(beta1)~ ", " ~
 sigma ~"="~ .(sigma2)),
 cex.main=0.5, cex=0.5)
abline(sim_model2, col='steelblue', lwd=3)

plot(y3 ~ x, pch=16, col='darkorange',
 main =bquote(beta[0] ~ "="~.(beta1) ~ ", " ~
 beta[1] ~ "="~.(beta1)~ ", " ~
 sigma ~"="~ .(sigma3)),
 cex.main = 0.5, cex=0.5)
abline(sim_model3, col='steelblue', lwd=3)
```
> sigma가 작을수록 회귀직선에 더 오밀조밀하게 붙어있음...


\- $\hat{\beta_1}$이 정말로 평균이 $\beta_1$이고, 분산이 어쩌고인 정규분포를 따를까?

```{r}
num_samples = 10000

beta0_hats = rep(0, num_samples)
beta1_hats = rep(0, num_samples)  ## 일단 더미로 리스트를 만든듯?
## 만번 정도 회귀계수 추정치를 구해서 분포를 알아보자
```


```{r}
beta0 <- 10
beta1 <- -3
sigma <- 3

n <- 50
```

```{r}
set.seed(1004)  ## 천사가 좋으셨나봄
x <- runif(n = n) * 20  ## U(0,1)인 균일분포에다 20 곱함(근데 이럴거면 upper를 20으로...)
```

```{r}
temp_dt <- generating_slr(x, beta0, beta1, sigma)
m1 <- lm(y~x, temp_dt)
summary(m1)
```

```{r}
plot(y~x, temp_dt, pch = 16, col = 'orange',
     main = bquote(beta[0]~"="~.(beta0)~","~beta[1]~'='~.(beta1)~","~sigma~"="~.(sigma)), cex.main = 0.8)
abline(m1, col = 'red', lwd = 2)
## bquote로 문자열을 r-string처럼 활용할 수 있는듯. 그리고.format()을 .()로 사용할 수 있는듯(뇌피셜임). 그리고 ~는 문자열 간 나누는 것인듯
## 그럼 expression()하고 어떻게 용도가 다른거임??
```
```{r}
print(paste('123','345', sep = ""))
```
\- beta1만 뽑으려면

```{r}
m1$coefficients[2]
coef(m1)[2]
## 둘다 똑같음
names(m1)
```
```{r}
generating_slr = function(x, beta0, beta1, sigma) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta0 + beta1*x + epsilon
  data.frame(x, y)
}
## x와 그에 따른 y의 값을 난수로 만들어 데이터프레임으로 정리하는 함수
```

```{r}
set.seed(1004)

## beta1을 뽑을 때마다 sampling을 계속해서 하여 beta1이 비편향추정량임을 실제로 알아보자.

for (i in 1:num_samples) {
  temp_dt <- generating_slr(x, beta0, beta1, sigma)  ## temp data
  
  sim_fit = lm(y~x, data = temp_dt)
  
  beta1_hats[i] = sim_fit$coefficients[2]  ## 이미 만들어놨던 더미 리스트에 각 값을 beta1_hat으로 바꿈
}
```

```{r}
head(beta1_hats)
## > real beta1 : -3

mean(beta1_hats)
var(beta1_hats)
sd(beta1_hats)
```

```{r}
S_xx = sum((x-mean(x))**2)
beta1_hat_var = sigma**2 / S_xx
beta1_hat_var  ## beta1_hat의 실제 분산(모분산 이용함)
```

> simulation과 real과의 차이가 오차를 감안하면 같다.


```{r}
hist(beta1_hats, prob = TRUE, breaks = 50, xlab = expression(hat(beta)[1]), main = bquote(hat(beta[1])~'distribution'), labels = expression(hat(beta)[1]), col = 'skyblue')
curve(dnorm(x, mean = beta1, sd = sqrt(beta1_hat_var)), add = TRUE, col = 'orange', lwd = 3)  ## curve는 labeling이 안되나봐...
legend("topright", expression(hat(beta)[1]), title = "legend", fill = c('skyblue', 'orange'))
```


> 관측치들의 분포가 이론적 값과 유사하게 나왔다는 것을 알 수 있다.

* x값에 따른 결과의 차이(분모에 해당하는 차이)

```{r}
set.seed(1004)
sigma = 2
x1 <- runif(n = n)*5  ## runif(n = n, min = 0, max = 5), x값이 더욱 조밀하게 들어옴(분산이 작음)
```

```{r}
temp_dt2 <- generating_slr(x1, beta0, beta1, sigma)
head(temp_dt2)
```

```{r}
plot(y~x1, temp_dt2, pch = 16, col = 'darkorange', main = bquote(beta[0]~"="~.(beta0) ~ ", " ~beta[1] ~ "="~.(beta1)~ ", " ~sigma ~"="~ .(sigma)), cex.main = 0.7, cex = 0.7)
```


> 값들이 전보다 더 퍼져보임.

```{r}
m2 <- lm(y~x1, temp_dt2)
summary(m2)
```

> t-value도 더 크고, F-value도 더 크다.(0~20일 때는 각각 -45, 3000이었음)

```{r}
names(summary(m2))
names(m2)
```

```{r}
summary(m2)$coef  ## summary(m2)$coefficients, m2$coefficient는 계수만 산출함
summary(m1)$coef
```

> x의 범위가 넓어야 좋다. (데이터의 갯수를 늘려도 분산은 커지므로 데이터를 더 많이 모으면 좋아질 것)

* 모형에 대한 가정(선형성, 독립성, 등분산성, 정규성) -> LSE는 BLUE이다.

```{r}
## 등분산성 가정 불만족
generating_slr_hetero <- function(x, beta0, beta1) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = x)  ## x가 커질수록 오차항의 분산이 커짐
  y = beta0 + beta1*x + epsilon
  data.frame(x, y)
}
```

```{r}
## x는 U(0, 20)에서 무작위추출한 데이터임
temp_dt3 <- generating_slr_hetero(x, beta0, beta1)
head(temp_dt3)
```

```{r}
plot(y~x, data = temp_dt3, pch = 16, col = 'darkorange',
     main = expression(paste(beta[0], ' = 10,  ', beta[1], ' = -3,  ', sigma, ' = x')))

```
> x가 높아질수록 y가 벌어지고 있다.

```{r}
m2 <- lm(y~x, data = temp_dt3)
summary(m2)
```

> 표준오차가 훨씬 크다. F-value도 훨씬 작다.

```{r}
summary(m1)$coef  ## 등분산성, sigma = 2
summary(m2)$coef  ## 이분산성, sigma = x

summary(m1)$sigma  ## sqrt(MSE), 모든 가정 만족
summary(m2)$sigma  ## MSE가 훨씬 크다.
```

```{r}
beta1_hats_m2 <- rep(0, num_samples)


for (i in 1:num_samples) {
  temp_dt <- generating_slr_hetero(x, beta0, beta1)
  temp_lm <- lm(y~x, data = temp_dt)
  
  beta1_hats_m2[i] <- temp_lm$coefficient[2]
}

```

```{r}
var_beta1_hats2 <- var(beta1_hats_m2)
var_beta1_hats2

## 분산이 훨씬 커짐, 원래 0.04정도였나 그랬음.
```

```{r}
par(mfcol = c(1,2))

hist(beta1_hats_m2, prob = TRUE, breaks = 50)
curve(dnorm(x, mean = beta1, sd = sqrt(var_beta1_hats2)), add = TRUE,
      col = 'orange', lwd = 3)


hist(beta1_hats, prob = TRUE, breaks = 50)
curve(dnorm(x, mean = beta1, sd = sqrt(beta1_hat_var)), add = TRUE,
      col = 'orange', lwd = 3)

```

> 훨씬 더 beta의 값들이 넓게 퍼져있는 것을 볼 수 있다.(range를 보면 스케일이 다름)